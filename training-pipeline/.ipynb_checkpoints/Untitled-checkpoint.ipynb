{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a91b3947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.34.38)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.34.50-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting botocore<1.35.0,>=1.34.50 (from boto3)\n",
      "  Downloading botocore-1.34.50-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.50->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.50->boto3) (1.26.18)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.50->boto3) (1.16.0)\n",
      "Downloading boto3-1.34.50-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.34.50-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: botocore, boto3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.34.38\n",
      "    Uninstalling botocore-1.34.38:\n",
      "      Successfully uninstalled botocore-1.34.38\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.34.38\n",
      "    Uninstalling boto3-1.34.38:\n",
      "      Successfully uninstalled boto3-1.34.38\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.32.38 requires botocore==1.34.38, but you have botocore 1.34.50 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.34.50 botocore-1.34.50\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b491db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5b97b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "pipeline_name = f\"sagemaker-mlops-train-pipeline\"\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "s3_client = boto3.resource('s3')\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "pipeline_session = PipelineSession()\n",
    "default_bucket= sagemaker_session.default_bucket()\n",
    "model_package_group_name = f\"ChurnModelPackageGroup\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc347dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "ParameterInteger,\n",
    "ParameterString,\n",
    "ParameterFloat)\n",
    "\n",
    "auc_score_threshold = 0.75\n",
    "base_job_prefix = 'churn-example'\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "processing_instance_type = ParameterString( name=\"ProcessingInstanceType\", default_value=\"ml.m5.xlarge\") \n",
    "training_instance_type = ParameterString( name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\") \n",
    "model_approval_status = ParameterString( name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8870cb6",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d36e0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-26 20:56:45--  https://raw.githubusercontent.com/manifoldailearning/mlops-with-aws-datascientists/main/Section-16-mlops-pipeline/dataset/storedata_total.xlsx\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2713209 (2.6M) [application/octet-stream]\n",
      "Saving to: ‘storedata_total.xlsx’\n",
      "\n",
      "100%[======================================>] 2,713,209   --.-K/s   in 0.02s   \n",
      "\n",
      "2024-02-26 20:56:46 (170 MB/s) - ‘storedata_total.xlsx’ saved [2713209/2713209]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/manifoldailearning/mlops-with-aws-datascientists/main/Section-16-mlops-pipeline/dataset/storedata_total.xlsx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bea88a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/openpyxl/worksheet/_read_only.py:81: UserWarning: Unknown extension is not supported and will be removed\n",
      "  for idx, row in parser.parse():\n"
     ]
    }
   ],
   "source": [
    "store_data = pd.read_excel('storedata_total.xlsx')\n",
    "store_data.to_csv(\"storedata_total.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a09755",
   "metadata": {},
   "source": [
    "## Processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b888d091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-26 20:58:50--  https://raw.githubusercontent.com/manifoldailearning/mlops-with-aws-datascientists/main/Section-16-mlops-pipeline/preprocess-churn.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2156 (2.1K) [text/plain]\n",
      "Saving to: ‘preprocess-churn.py’\n",
      "\n",
      "100%[======================================>] 2,156       --.-K/s   in 0s      \n",
      "\n",
      "2024-02-26 20:58:50 (40.4 MB/s) - ‘preprocess-churn.py’ saved [2156/2156]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/manifoldailearning/mlops-with-aws-datascientists/main/Section-16-mlops-pipeline/preprocess-churn.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd52d666",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtempfile\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mdt\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "    base_dir = \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#Read Data\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    df = pd.read_csv(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbase_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/input/storedata_total.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# convert created column to datetime\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    df[\u001b[33m\"\u001b[39;49;00m\u001b[33mcreated\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = pd.to_datetime(df[\u001b[33m\"\u001b[39;49;00m\u001b[33mcreated\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#Convert firstorder and lastorder to datetime datatype\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    df[\u001b[33m\"\u001b[39;49;00m\u001b[33mfirstorder\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = pd.to_datetime(df[\u001b[33m\"\u001b[39;49;00m\u001b[33mfirstorder\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],errors=\u001b[33m'\u001b[39;49;00m\u001b[33mcoerce\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    df[\u001b[33m\"\u001b[39;49;00m\u001b[33mlastorder\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = pd.to_datetime(df[\u001b[33m\"\u001b[39;49;00m\u001b[33mlastorder\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],errors=\u001b[33m'\u001b[39;49;00m\u001b[33mcoerce\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#Drop Rows with Null Values\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    df = df.dropna()\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#Create column which gives the days between the last order and the first order\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    df[\u001b[33m'\u001b[39;49;00m\u001b[33mfirst_last_days_diff\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = (df[\u001b[33m'\u001b[39;49;00m\u001b[33mlastorder\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] - df[\u001b[33m'\u001b[39;49;00m\u001b[33mfirstorder\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]).dt.days\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#Create column which gives the days between the customer record was created and the first order\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    df[\u001b[33m'\u001b[39;49;00m\u001b[33mcreated_first_days_diff\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = (df[\u001b[33m'\u001b[39;49;00m\u001b[33mcreated\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] - df[\u001b[33m'\u001b[39;49;00m\u001b[33mfirstorder\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]).dt.days\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#Drop columns\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    df.drop([\u001b[33m'\u001b[39;49;00m\u001b[33mcustid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mcreated\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[33m'\u001b[39;49;00m\u001b[33mfirstorder\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[33m'\u001b[39;49;00m\u001b[33mlastorder\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], axis=\u001b[34m1\u001b[39;49;00m, inplace=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#Apply one hot encoding on favday and city columns\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    df = pd.get_dummies(df, prefix=[\u001b[33m'\u001b[39;49;00m\u001b[33mfavday\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mcity\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], columns=[\u001b[33m'\u001b[39;49;00m\u001b[33mfavday\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mcity\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Split into train, validation and test datasets\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    y = df.pop(\u001b[33m\"\u001b[39;49;00m\u001b[33mretained\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    X_pre = df\u001b[37m\u001b[39;49;00m\n",
      "    y_pre = y.to_numpy().reshape(\u001b[36mlen\u001b[39;49;00m(y), \u001b[34m1\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    X = np.concatenate((y_pre, X_pre), axis=\u001b[34m1\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    np.random.shuffle(X)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Split in Train, Test and Validation Datasets\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    train, validation, test = np.split(X, [\u001b[36mint\u001b[39;49;00m(\u001b[34m.7\u001b[39;49;00m*\u001b[36mlen\u001b[39;49;00m(X)), \u001b[36mint\u001b[39;49;00m(\u001b[34m.85\u001b[39;49;00m*\u001b[36mlen\u001b[39;49;00m(X))])\u001b[37m\u001b[39;49;00m\n",
      "    train_rows = np.shape(train)[\u001b[34m0\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "    validation_rows = np.shape(validation)[\u001b[34m0\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "    test_rows = np.shape(test)[\u001b[34m0\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "    train = pd.DataFrame(train)\u001b[37m\u001b[39;49;00m\n",
      "    test = pd.DataFrame(test)\u001b[37m\u001b[39;49;00m\n",
      "    validation = pd.DataFrame(validation)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Convert the label column to integer\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    train[\u001b[34m0\u001b[39;49;00m] = train[\u001b[34m0\u001b[39;49;00m].astype(\u001b[36mint\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    test[\u001b[34m0\u001b[39;49;00m] = test[\u001b[34m0\u001b[39;49;00m].astype(\u001b[36mint\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    validation[\u001b[34m0\u001b[39;49;00m] = validation[\u001b[34m0\u001b[39;49;00m].astype(\u001b[36mint\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Save the Dataframes as csv files\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    train.to_csv(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbase_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/train/train.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, header=\u001b[34mFalse\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    validation.to_csv(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbase_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/validation/validation.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, header=\u001b[34mFalse\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    test.to_csv(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbase_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/test/test.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, header=\u001b[34mFalse\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize \"preprocess-churn.py\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75c71770",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"storedata_total.csv\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "039cd5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b548b336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "framework_version = \"1.0-1\"\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "        framework_version=framework_version,\n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"sklearn-churn-process\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "processor_args = sklearn_processor.run(\n",
    "    inputs=[\n",
    "      ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),  \n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\",\\\n",
    "                         destination=f\"s3://{default_bucket}/output/train\" ),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\",\\\n",
    "                        destination=f\"s3://{default_bucket}/output/validation\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\",\\\n",
    "                        destination=f\"s3://{default_bucket}/output/test\")\n",
    "    ],\n",
    "    code=f\"preprocess-churn.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5042fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_process = ProcessingStep(name=\"ChurnModelProcess\", step_args=processor_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465c1e7e",
   "metadata": {},
   "source": [
    "## Hyperparam Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67bbc880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.tuner import (\n",
    "        IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner)\n",
    "from sagemaker.workflow.steps import TuningStep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a19d140d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is interpreted in pipeline execution time only. As the function needs to evaluate the argument value in SDK compile time, the default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n"
     ]
    }
   ],
   "source": [
    "model_path = f\"s3://{default_bucket}/output\"\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.0-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=training_instance_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd1c66da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_hyperparameters = {\n",
    "\"eval_metric\":\"auc\",\n",
    "\"objective\":\"binary:logistic\",\n",
    "\"num_round\":\"100\",\n",
    "\"rate_drop\":\"0.3\",\n",
    "\"tweedie_variance_power\":\"1.4\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "619d4ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    hyperparameters=fixed_hyperparameters,\n",
    "    output_path=model_path,\n",
    "    base_job_name=f\"churn-train\",\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "457fb099",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "\"eta\": ContinuousParameter(0, 1),\n",
    "\"min_child_weight\": ContinuousParameter(1, 10),\n",
    "\"alpha\": ContinuousParameter(0, 2),\n",
    "\"max_depth\": IntegerParameter(4, 10),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd226da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = \"validation:auc\"\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    xgb_train,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    max_jobs=2,\n",
    "    max_parallel_jobs=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef5e0dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_args = tuner.fit(\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"validation\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03f9d669",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_tuning = TuningStep(\n",
    "    name=\"ChurnHyperParameterTuning\",\n",
    "    step_args=hpo_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aa072e",
   "metadata": {},
   "source": [
    "## Evaluation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a12e77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-26 21:32:05--  https://raw.githubusercontent.com/manifoldailearning/mlops-with-aws-datascientists/main/Section-16-mlops-pipeline/evaluate-churn.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1309 (1.3K) [text/plain]\n",
      "Saving to: ‘evaluate-churn.py’\n",
      "\n",
      "100%[======================================>] 1,309       --.-K/s   in 0s      \n",
      "\n",
      "2024-02-26 21:32:05 (52.6 MB/s) - ‘evaluate-churn.py’ saved [1309/1309]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/manifoldailearning/mlops-with-aws-datascientists/main/Section-16-mlops-pipeline/evaluate-churn.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa0e98b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "script_eval = ScriptProcessor(\n",
    "    image_uri=image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"script-churn-eval\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f13ff7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_args = script_eval.run(\n",
    "     inputs=[\n",
    "            ProcessingInput(\n",
    "                source=step_tuning.get_top_model_s3_uri(top_k=0,s3_bucket=default_bucket,prefix=\"output\"),\n",
    "                destination=\"/opt/ml/processing/model\"\n",
    "            ),\n",
    "            ProcessingInput(\n",
    "                source=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"test\"\n",
    "                ].S3Output.S3Uri,\n",
    "                destination=\"/opt/ml/processing/test\"\n",
    "            )\n",
    "        ],\n",
    "    outputs=[\n",
    "            ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\",\\\n",
    "                             destination=f\"s3://{default_bucket}/output/evaluation\"),\n",
    "        ],\n",
    "    code=f\"evaluate-churn.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8afdc210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"ChurnEvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n",
    ")\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"ChurnEvalModel\",\n",
    "    step_args=eval_args,\n",
    "    property_files=[evaluation_report])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e7b5bc",
   "metadata": {},
   "source": [
    "## Define a register model step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f3f3386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import Model\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "model = Model(image_uri= image_uri,\n",
    "    model_data=step_tuning.get_top_model_s3_uri(top_k=0,s3_bucket=default_bucket,prefix=\"output\"),\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08458230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8339d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
